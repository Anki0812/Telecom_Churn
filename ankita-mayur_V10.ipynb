{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":57.533667,"end_time":"2021-08-13T07:17:27.204430","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2021-08-13T07:16:29.670763","version":"2.3.3"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Telecom Churn Prediction","metadata":{"papermill":{"duration":0.036252,"end_time":"2021-08-13T07:16:36.548737","exception":false,"start_time":"2021-08-13T07:16:36.512485","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Problem statement\n\nIn the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n\nFor many incumbent operators, retaining high profitable customers is the number one business\ngoal. To reduce customer churn, telecom companies need to predict which customers are at high risk of churn. In this project, you will analyze customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn, and identify the main indicators of churn.\n\nIn this competition, your goal is *to build a machine learning model that is able to predict churning customers based on the features provided for their usage.*\n\n**Customer behaviour during churn:**\n\nCustomers usually do not decide to switch to another competitor instantly, but rather over a\nperiod of time (this is especially applicable to high-value customers). In churn prediction, we\nassume that there are three phases of customer lifecycle :\n\n1. <u>The ‘good’ phase:</u> In this phase, the customer is happy with the service and behaves as usual.\n\n2. <u>The ‘action’ phase:</u> The customer experience starts to sore in this phase, for e.g. he/she gets a compelling offer from a competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the ‘good’ months. It is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor’s offer/improving the service quality etc.)\n\n3. <u>The ‘churn’ phase:</u> In this phase, the customer is said to have churned. In this case, since you are working over a four-month window, the first two months are the ‘good’ phase, the third month is the ‘action’ phase, while the fourth month (September) is the ‘churn’ phase.","metadata":{"papermill":{"duration":0.034552,"end_time":"2021-08-13T07:16:36.690028","exception":false,"start_time":"2021-08-13T07:16:36.655476","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Below steps will be folowed for evaluation\n1. Loading dependencies & datasets","metadata":{}},{"cell_type":"markdown","source":"# 1. Loading dependencies & datasets\n\nLets start by loading our dependencies. We can keep adding any imports to this cell block, as we write mode and mode code.","metadata":{"papermill":{"duration":0.034501,"end_time":"2021-08-13T07:16:36.760335","exception":false,"start_time":"2021-08-13T07:16:36.725834","status":"completed"},"tags":[]}},{"cell_type":"code","source":"pip install missingno","metadata":{"execution":{"iopub.status.busy":"2023-10-27T06:05:54.202370Z","iopub.execute_input":"2023-10-27T06:05:54.202822Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: missingno in /opt/conda/lib/python3.10/site-packages (0.5.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from missingno) (1.23.5)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from missingno) (3.7.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from missingno) (1.11.2)\nRequirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (from missingno) (0.12.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->missingno) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->missingno) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->missingno) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->missingno) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->missingno) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->missingno) (9.5.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->missingno) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->missingno) (2.8.2)\nRequirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.10/site-packages (from seaborn->missingno) (2.0.3)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->seaborn->missingno) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->seaborn->missingno) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->missingno) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"#Data Structures\nimport pandas as pd\nimport numpy as np\nimport re\nimport os\n\n### For installing missingno library, type this command in terminal\n#pip install missingno\n\nimport missingno as msno\n\n#Sklearn\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score\n\n#Plotting\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\n\n# Class Imbalance\nfrom imblearn.over_sampling import SMOTE\n\n#Others\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","metadata":{"papermill":{"duration":1.362112,"end_time":"2021-08-13T07:16:38.158342","exception":false,"start_time":"2021-08-13T07:16:36.796230","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we load our datasets and the data dictionary file.\n\nThe **train.csv** file contains both dependent and independent features, while the **test.csv** contains only the independent variables. \n\nSo, for model selection, I will create our own train/test dataset from the **train.csv** and use the model to predict the solution using the features in unseen test.csv data for submission.","metadata":{"papermill":{"duration":0.03468,"end_time":"2021-08-13T07:16:38.240579","exception":false,"start_time":"2021-08-13T07:16:38.205899","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Derived attribute related functions","metadata":{}},{"cell_type":"code","source":"#The above data is with respect to data recharge\n# We can create new column indicating data recharge and substitute null columns with 0 value\n\n\ndef create_data_rechange_info(df):\n    df['data_rechange_6'] = df['date_of_last_rech_data_6'].isnull()\n    df['data_rechange_7'] = df['date_of_last_rech_data_7'].isnull()\n    df['data_rechange_8'] = df['date_of_last_rech_data_8'].isnull()\n\n    del df['date_of_last_rech_data_6']\n    del df['date_of_last_rech_data_7']\n    del df['date_of_last_rech_data_8']\n\n    df_len = len(df)*0.7\n    i=0\n    for col in df.columns:\n        if df[col].isnull().sum() >= df_len:\n            df[col].fillna(0)\n    return df\n\ndef create_last_rechange_days(df):\n    df['last_recharge_days'] = (pd.to_datetime('2014-08-31') - df[['date_of_last_rech_6', 'date_of_last_rech_7', 'date_of_last_rech_8']].apply(pd.to_datetime).max(axis=1)).dt.days\n    del df['date_of_last_rech_6']\n    del df['date_of_last_rech_7']\n    del df['date_of_last_rech_8']\n    return df\n\ndef create_new_variables(df):\n    df = create_data_rechange_info(df)\n    df = create_last_rechange_days(df)\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set display limits\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)","metadata":{}},{"cell_type":"code","source":"#COMMENT THIS SECTION INCASE RUNNING THIS NOTEBOOK LOCALLY\n\n#Checking the kaggle paths for the uploaded datasets\n#import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"papermill":{"duration":0.044801,"end_time":"2021-08-13T07:16:38.320264","exception":false,"start_time":"2021-08-13T07:16:38.275463","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#INCASE RUNNING THIS LOCALLY, PASS THE RELATIVE PATH OF THE CSV FILES BELOW\n#(e.g. if files are in same folder as notebook, simple write \"train.csv\" as path)\n\ndata = pd.read_csv(\"/kaggle/input/telecom-churn-case-study-hackathon-c53/train.csv\")\nunseen = pd.read_csv(\"/kaggle/input/telecom-churn-case-study-hackathon-c53/test.csv\")\nsample = pd.read_csv(\"/kaggle/input/telecom-churn-case-study-hackathon-c53/sample.csv\")\ndata_dict = pd.read_csv(\"/kaggle/input/telecom-churn-case-study-hackathon-c53/data_dictionary.csv\")\n\n#data = pd.read_csv(\"train.csv\")\n#unseen = pd.read_csv(\"test.csv\")\n#sample = pd.read_csv(\"sample.csv\")\n#data_dict = pd.read_csv(\"data_dictionary.csv\")\n\n\nprint('Train data shape - ',data.shape)\nprint('Test data shape - ',unseen.shape)\nprint('Sample data shape - ',sample.shape)\nprint('Data dictionary shape - ', data_dict.shape)","metadata":{"papermill":{"duration":2.591587,"end_time":"2021-08-13T07:16:40.948543","exception":false,"start_time":"2021-08-13T07:16:38.356956","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check data type of columns","metadata":{}},{"cell_type":"code","source":"data.info()\n# data type are correctly assigned","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Columns with NULL value","metadata":{}},{"cell_type":"code","source":"## Columns with Null values\nfiltered_data = data.loc[:, data.isnull().sum() != 0]\nfiltered_data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Data cleaning and formatting","metadata":{}},{"cell_type":"markdown","source":"### Delete columns with only one value","metadata":{}},{"cell_type":"code","source":"df_len = len(data)*0.9\ni=0\nfor col in data.columns:\n    if len(data[col].value_counts()) == 1:\n        print(data[col].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[data['loc_og_t2o_mou'].isnull()]['churn_probability'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_len = len(data)*0.9\ni=0\nfor col in data.columns:\n    if len(data[col].value_counts()) == 1:\n        i=i+1\n        print(i,'.', col,'.',data[col][0],'.',round(data[col].value_counts().iloc[0]*100/len(data),2),'%')\n        del data[col]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Delete columns with more than 90% same values","metadata":{}},{"cell_type":"code","source":"df_len = len(data)*0.9\ni=0\nl = []\nfor col in data.columns:\n    if data[col].value_counts().iloc[0] >= df_len:\n        i=i+1\n        print(i,'.', col,'.',data[col].value_counts().iloc[0],'.',round(data[col].value_counts().iloc[0]*100/len(data),2),'%')\n        del data[col]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing columns with more than 70 % null values","metadata":{}},{"cell_type":"code","source":"df_len = len(data)*0.7\ni=0\nprint('Columns with more than 70% missing values')\nfor col in data.columns:\n    if data[col].isnull().sum() >= df_len:\n        i=i+1\n        print(i,'.', col,'.',data[col].isnull().sum(),'.', round((data[col].isnull().sum()/len(data))*100,2))\n        #del data[col]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <span style=\"color:blue\">Observation :-</span>\nSince same count of data for rechange related columns are missing, hence will not delete it.\nWill handle in derived data section.\nCreate new attribute to indicate has data recharge or not and consider default value for NA cell as Zero.","metadata":{}},{"cell_type":"code","source":"data = create_data_rechange_info(data)\nunseen = create_data_rechange_info(unseen)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing columns not required for analysis","metadata":{}},{"cell_type":"code","source":"del data['id'] # primary key","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing rows with all null values in July, Aug, Sept","metadata":{}},{"cell_type":"code","source":"# Get all unique prefixes from column names\nprefixes = set('_'.join(col.split('_')[0:-1]) for col in data.columns if col.split('_')[-1] in ['6','7','8'])\n# Create a list to store filtered DataFrames for each prefix\nfiltered_dfs = []\nprint('Shape before row delete - ', data.shape)\n# Iterate over prefixes and filter rows where all columns with the same prefix are null\nfor prefix in prefixes:\n    # Get columns with the current prefix\n    cols_with_prefix = [col for col in data.columns if col.startswith(prefix)]\n    \n    # Filter rows where all columns with the same prefix are null\n    filtered_df = data[data[cols_with_prefix].isnull().all(axis=1)]\n    if len(filtered_df) > 0:\n        print(prefix,'_*', ' count - ', len(filtered_df))\n    # Append the filtered DataFrame to the list\n    data = data[~data[cols_with_prefix].isnull().all(axis=1)]\n    \nprint('Shape after row delete - ', data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check for duplicates","metadata":{}},{"cell_type":"code","source":"data[data.duplicated()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add default value 0 to numerical values","metadata":{}},{"cell_type":"code","source":"for col in data.select_dtypes(include=['float']):\n    data[col] = data[col].fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filtered_data = data.loc[:, data.isnull().sum() != 0]\nfiltered_data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Derived attributes","metadata":{}},{"cell_type":"code","source":"df_len = len(data)*0.7\ni=0\nprint('Columns with more than 50% missing values deleted')\nfor col in data.columns:\n    if data[col].isnull().sum() >= df_len:\n        i=i+1\n        print(i,'.', col,'.',data[col].isnull().sum(),'.', round((data[col].isnull().sum()/len(data))*100,2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <span style=\"color:blue\">Observation :-</span>\nThe value in above columns is only for data rechange related columns\nSo we can create a column to depict data recharge and fill null numerical values with 0 instead of NA","metadata":{}},{"cell_type":"markdown","source":"### derive last recharge in days from dates","metadata":{}},{"cell_type":"code","source":"data = create_last_rechange_days(data)\nunseen = create_last_rechange_days(unseen)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Columns with Null values\nfiltered_data = data.loc[:, data.isnull().sum() != 0]\nfiltered_data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[data['onnet_mou_8'].isnull()]['churn_probability'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['churn_probability'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Columns with Null values\nfiltered_data = unseen.loc[:, unseen.isnull().sum() != 0]\nfiltered_data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Exploratory Data analysis ","metadata":{}},{"cell_type":"markdown","source":"### Univariant analysis","metadata":{}},{"cell_type":"code","source":"plt.pie(data['churn_probability'].value_counts().values,labels=['Not Churn','Churn'], autopct='%.2f%%')\nplt.axis('equal')\nplt.title('Churn vs Not Churn')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> <span style=\"color:blue\">Observation</span> - Only 10% of data has churn related information hence class imbalance handling techniques to be used","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get all unique prefixes from column names\nmonth_columns = list(col for col in data.columns if 'total' in col)\n\n# figure size\nplt.figure(figsize=(20, 15))\n\n# heatmap\nsns.heatmap(data[month_columns].corr(), cmap=\"Wistia\", annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drop month related columns which are highly correlated with total","metadata":{}},{"cell_type":"code","source":"correlation_matrix = data.corr()\n\n# Set the correlation threshold (adjust as needed)\ncorrelation_threshold = 0.7\ntotal_columns = set(col for col in data.columns if 'total' in col)\nmonth_columns = set(col for col in data.columns if 'total' not in col and col.split('_')[-1] in ['6','7','8'])\n\n# Find columns with high correlation\nhighly_correlated_columns = set()\nfor i in total_columns:\n    for j in month_columns:\n        if abs(correlation_matrix[i][j]) > correlation_threshold:\n            print(j,\" --- \",correlation_matrix[i][j])\n            highly_correlated_columns.add(j)\n\n# 'highly_correlated_columns' now contains the names of columns with high correlation\nprint(highly_correlated_columns)\n\ndata.drop(columns=highly_correlated_columns, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drop month related columns which are highly correlated","metadata":{}},{"cell_type":"markdown","source":"correlation_matrix = data.corr()\n\n# Set the correlation threshold (adjust as needed)\ncorrelation_threshold = 0.8\nmonth_columns = set(col for col in data.columns if 'total' not in col and col.split('_')[-1] in ['6','7','8'])\n\n# Find columns with high correlation\nhighly_correlated_columns = set()\nfor i in month_columns:\n    for j in month_columns:\n        if abs(correlation_matrix[i][j]) > correlation_threshold and i!=j:\n            highly_correlated_columns.add(j)\n\n# 'highly_correlated_columns' now contains the names of columns with high correlation\nprint(highly_correlated_columns)\n\ndata.drop(columns=highly_correlated_columns, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-24T09:18:36.819307Z","iopub.execute_input":"2023-10-24T09:18:36.819680Z","iopub.status.idle":"2023-10-24T09:18:38.539050Z","shell.execute_reply.started":"2023-10-24T09:18:36.819648Z","shell.execute_reply":"2023-10-24T09:18:38.537867Z"}}},{"cell_type":"markdown","source":"correlation_matrix = data.corr()\n\n# Set the correlation threshold (adjust as needed)\ncorrelation_threshold = 0.80\nmonth_columns = set(col for col in data.columns if 'total' not in col and col.split('_')[-1] in ['6','7','8'])\n\n# Find columns with high correlation\nhighly_correlated_columns = set()\nfor i in data.columns:\n    for j in data.columns:\n        if abs(correlation_matrix[i][j]) > correlation_threshold and i!=j:\n            print(j,\" --- \",correlation_matrix[i][j])\n            highly_correlated_columns.add(j)\n\n# 'highly_correlated_columns' now contains the names of columns with high correlation\nprint(highly_correlated_columns)\n\ndata.drop(columns=highly_correlated_columns, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-24T10:41:08.946167Z","iopub.execute_input":"2023-10-24T10:41:08.947130Z","iopub.status.idle":"2023-10-24T10:41:11.054462Z","shell.execute_reply.started":"2023-10-24T10:41:08.947093Z","shell.execute_reply":"2023-10-24T10:41:11.053362Z"}}},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dict","metadata":{"papermill":{"duration":0.067353,"end_time":"2021-08-13T07:16:41.123346","exception":false,"start_time":"2021-08-13T07:16:41.055993","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the purpose of this **starter notebook**, we I will restrict the dataset to only a small set of variables. \n\nThe approach I use here is to understand each Acronym, figure our what variable might be important and filter out variable names based on the combinations of acrynoms using REGEX. So, if I want the total minutes a person has spent on outgoing calls, I need acronyms, TOTAL, OG and MOU. So corresponding regex is ```total.+og.+mou```","metadata":{"papermill":{"duration":0.036844,"end_time":"2021-08-13T07:16:41.198447","exception":false,"start_time":"2021-08-13T07:16:41.161603","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data.head()","metadata":{"papermill":{"duration":0.066079,"end_time":"2021-08-13T07:16:41.401769","exception":false,"start_time":"2021-08-13T07:16:41.335690","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at each variable's datatype:","metadata":{"papermill":{"duration":0.037694,"end_time":"2021-08-13T07:16:41.476864","exception":false,"start_time":"2021-08-13T07:16:41.439170","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data.info(verbose=1)","metadata":{"papermill":{"duration":0.064419,"end_time":"2021-08-13T07:16:41.578526","exception":false,"start_time":"2021-08-13T07:16:41.514107","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's also summarize the features using the df.describe method:","metadata":{"papermill":{"duration":0.037487,"end_time":"2021-08-13T07:16:41.653911","exception":false,"start_time":"2021-08-13T07:16:41.616424","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data.describe(include=\"all\")","metadata":{"papermill":{"duration":0.160583,"end_time":"2021-08-13T07:16:41.852218","exception":false,"start_time":"2021-08-13T07:16:41.691635","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Create X, y and then Train test split\n\nLets create X and y datasets and skip \"circle_id\" since it has only 1 unique value","metadata":{"papermill":{"duration":0.03869,"end_time":"2021-08-13T07:16:41.929599","exception":false,"start_time":"2021-08-13T07:16:41.890909","status":"completed"},"tags":[]}},{"cell_type":"code","source":"y = data.pop('churn_probability')\nX = data\n\nX.shape, y.shape","metadata":{"papermill":{"duration":0.054463,"end_time":"2021-08-13T07:16:42.111249","exception":false,"start_time":"2021-08-13T07:16:42.056786","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting train and test data to avoid any contamination of the test data","metadata":{"papermill":{"duration":0.039731,"end_time":"2021-08-13T07:16:42.189842","exception":false,"start_time":"2021-08-13T07:16:42.150111","status":"completed"},"tags":[]}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","metadata":{"papermill":{"duration":0.065525,"end_time":"2021-08-13T07:16:42.294433","exception":false,"start_time":"2021-08-13T07:16:42.228908","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"papermill":{"duration":0.068615,"end_time":"2021-08-13T07:16:42.402101","exception":false,"start_time":"2021-08-13T07:16:42.333486","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Exploratory Data Analysis & Preprocessing\n\nLets start by analysing the univariate distributions of each feature.","metadata":{"papermill":{"duration":0.047951,"end_time":"2021-08-13T07:16:47.301731","exception":false,"start_time":"2021-08-13T07:16:47.253780","status":"completed"},"tags":[]}},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\nplt.xticks(rotation=45)\nsns.boxplot(data = X_train)","metadata":{"papermill":{"duration":0.885464,"end_time":"2021-08-13T07:16:48.234798","exception":false,"start_time":"2021-08-13T07:16:47.349334","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.1 Handling outliers\n\nThe box plots of these features show there a lot of outliers. These can be capped with k-sigma method.","metadata":{"papermill":{"duration":0.050148,"end_time":"2021-08-13T07:16:48.336611","exception":false,"start_time":"2021-08-13T07:16:48.286463","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def cap_outliers(array, k=3):\n    upper_limit = array.mean() + k*array.std()\n    lower_limit = array.mean() - k*array.std()\n    array[array<lower_limit] = lower_limit\n    array[array>upper_limit] = upper_limit\n    return array","metadata":{"papermill":{"duration":0.058225,"end_time":"2021-08-13T07:16:48.444315","exception":false,"start_time":"2021-08-13T07:16:48.386090","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_filtered1 = X_train.apply(cap_outliers, axis=0)\n\nplt.figure(figsize=(15,8))\nplt.xticks(rotation=45)\nsns.boxplot(data = X_train_filtered1)","metadata":{"papermill":{"duration":0.995019,"end_time":"2021-08-13T07:16:49.488905","exception":false,"start_time":"2021-08-13T07:16:48.493886","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2 Feature scaling\n\nLets also scale the features by scaling them with Standard scaler (few other alternates are min-max scaling and Z-scaling).","metadata":{"papermill":{"duration":0.050821,"end_time":"2021-08-13T07:16:49.590800","exception":false,"start_time":"2021-08-13T07:16:49.539979","status":"completed"},"tags":[]}},{"cell_type":"code","source":"new_vars = X_train_filtered1.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scale = StandardScaler()\nX_train_filtered2 = scale.fit_transform(X_train_filtered1)","metadata":{"papermill":{"duration":0.077522,"end_time":"2021-08-13T07:16:49.720148","exception":false,"start_time":"2021-08-13T07:16:49.642626","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\nplt.xticks(rotation=45)\nsns.boxplot(data = pd.DataFrame(X_train_filtered2, columns=new_vars))","metadata":{"papermill":{"duration":0.893696,"end_time":"2021-08-13T07:16:50.665247","exception":false,"start_time":"2021-08-13T07:16:49.771551","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can perform feature transformations at this stage. \n\n1. **Positively skewed:** Common transformations of this data include square root, cube root, and log.\n2. **Negatively skewed:** Common transformations include square, cube root and logarithmic.\n\nPlease read the following link to understand how to perform feature scaling and preprocessing : https://scikit-learn.org/stable/modules/preprocessing.html\n \nLets also plot the correlations for each feature for bivariate analysis.","metadata":{"papermill":{"duration":0.053477,"end_time":"2021-08-13T07:16:50.772231","exception":false,"start_time":"2021-08-13T07:16:50.718754","status":"completed"},"tags":[]}},{"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.heatmap(pd.DataFrame(X_train_filtered2, columns=new_vars).corr())","metadata":{"papermill":{"duration":0.568818,"end_time":"2021-08-13T07:16:51.394261","exception":false,"start_time":"2021-08-13T07:16:50.825443","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Distribution for the churn probability\nsns.histplot(y_train)","metadata":{"papermill":{"duration":0.393552,"end_time":"2021-08-13T07:16:51.843093","exception":false,"start_time":"2021-08-13T07:16:51.449541","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Feature engineering and selection\n\nLet's understand feature importances for raw features as well as components to decide top features for modelling.","metadata":{"papermill":{"duration":0.054582,"end_time":"2021-08-13T07:16:51.952415","exception":false,"start_time":"2021-08-13T07:16:51.897833","status":"completed"},"tags":[]}},{"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\nrf.fit(X_train_filtered2, y_train)","metadata":{"papermill":{"duration":8.035233,"end_time":"2021-08-13T07:17:00.041999","exception":false,"start_time":"2021-08-13T07:16:52.006766","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importances = pd.DataFrame({'col':new_vars, 'importance':rf.feature_importances_})","metadata":{"papermill":{"duration":0.165618,"end_time":"2021-08-13T07:17:00.262949","exception":false,"start_time":"2021-08-13T07:17:00.097331","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\nplt.xticks(rotation=45)\nplt.bar(feature_importances['col'], feature_importances['importance'])","metadata":{"papermill":{"duration":0.295789,"end_time":"2021-08-13T07:17:00.614769","exception":false,"start_time":"2021-08-13T07:17:00.318980","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At this step, you can create a bunch of features based on business understanding, such as \n1. \"average % gain of 3g volume from month 6 to 8\" - (growth or decline of 3g usage month over month?)\n2. \"ratio of total outgoing amount and age of user on network\" - (average daily usage of a user?)\n3. \"standard deviation of the total amount paid by user for all services\" - (too much variability in charges?)\n4. etc..\n\nAnother way of finding good features would be to project them into a lower dimensional space using PCA. PCA creates components which are a linear combination of the features. This then allows you to select components which explain the highest amount of variance.\n\nLets try to project the data onto 2D space and plot. **Note:** you can try TSNE, which is another dimensionality reduction approach as well. Check https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html for moree details.","metadata":{"papermill":{"duration":0.055776,"end_time":"2021-08-13T07:17:00.727093","exception":false,"start_time":"2021-08-13T07:17:00.671317","status":"completed"},"tags":[]}},{"cell_type":"code","source":"pca = PCA(0.95)\npca_components = pca.fit_transform(X_train_filtered2)\nsns.scatterplot(x=pca_components[:,0], y=pca_components[:,1], hue=y_train)","metadata":{"papermill":{"duration":4.617268,"end_time":"2021-08-13T07:17:05.400295","exception":false,"start_time":"2021-08-13T07:17:00.783027","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x=pca_components[:,1], y=pca_components[:,2], hue=y_train)","metadata":{"papermill":{"duration":2.631052,"end_time":"2021-08-13T07:17:08.093002","exception":false,"start_time":"2021-08-13T07:17:05.461950","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(rf.feature_importances_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_components.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's also check which of the components have high feature importances towards the end goal of churn prediction.","metadata":{"papermill":{"duration":0.063551,"end_time":"2021-08-13T07:17:08.224795","exception":false,"start_time":"2021-08-13T07:17:08.161244","status":"completed"},"tags":[]}},{"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\nrf.fit(pca_components, y_train)\n\nfeature_importances = pd.DataFrame({'col':['component_'+str(i) for i in range(len(rf.feature_importances_))], \n                                    'importance':rf.feature_importances_})\n\nplt.figure(figsize=(15,8))\nplt.xticks(rotation=45)\nplt.bar(feature_importances['col'], feature_importances['importance'])","metadata":{"papermill":{"duration":12.694325,"end_time":"2021-08-13T07:17:20.982792","exception":false,"start_time":"2021-08-13T07:17:08.288467","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Model building\n\nLet's build a quick model with logistic regression and the first 2 PCA components.","metadata":{"papermill":{"duration":0.065189,"end_time":"2021-08-13T07:17:21.113066","exception":false,"start_time":"2021-08-13T07:17:21.047877","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Linear Regression","metadata":{}},{"cell_type":"markdown","source":"The steps of this pipeline would be the following, but this is only one type of pipeline -\n1. Imputation\n2. Scaling\n3. PCA\n4. Classification model\n","metadata":{"papermill":{"duration":0.064818,"end_time":"2021-08-13T07:17:21.699674","exception":false,"start_time":"2021-08-13T07:17:21.634856","status":"completed"},"tags":[]}},{"cell_type":"code","source":"imp = SimpleImputer(strategy='constant', fill_value=0)\nscale = StandardScaler()\npca = PCA(n_components=10, random_state=42)\nlr = LogisticRegression(max_iter=1000, tol=0.001,random_state=42)","metadata":{"papermill":{"duration":0.071699,"end_time":"2021-08-13T07:17:21.836592","exception":false,"start_time":"2021-08-13T07:17:21.764893","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe_lr = Pipeline(steps = [('imputation',imp),\n                         ('scaling',scale),\n                         ('pca',pca),\n                         ('model',lr)])","metadata":{"papermill":{"duration":0.073281,"end_time":"2021-08-13T07:17:21.975259","exception":false,"start_time":"2021-08-13T07:17:21.901978","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe_lr.fit(X_train[new_vars], y_train)","metadata":{"papermill":{"duration":0.485922,"end_time":"2021-08-13T07:17:22.526693","exception":false,"start_time":"2021-08-13T07:17:22.040771","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_score = pipe_lr.score(X_train[new_vars], y_train)\nprint(\"Training accuracy:\", train_score)","metadata":{"papermill":{"duration":0.111563,"end_time":"2021-08-13T07:17:22.755300","exception":false,"start_time":"2021-08-13T07:17:22.643737","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_score = pipe_lr.score(X_test[new_vars], y_test)\nprint(\"Test accuracy:\", test_score)","metadata":{"papermill":{"duration":0.087891,"end_time":"2021-08-13T07:17:22.960566","exception":false,"start_time":"2021-08-13T07:17:22.872675","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's make a confusion matrix to analyze how each class is being predicted by the model.","metadata":{"papermill":{"duration":0.070399,"end_time":"2021-08-13T07:17:23.152879","exception":false,"start_time":"2021-08-13T07:17:23.082480","status":"completed"},"tags":[]}},{"cell_type":"code","source":"confusion_matrix(y_train, pipe_lr.predict(X_train[new_vars]))","metadata":{"papermill":{"duration":0.200569,"end_time":"2021-08-13T07:17:23.421831","exception":false,"start_time":"2021-08-13T07:17:23.221262","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test, pipe_lr.predict(X_test[new_vars]))","metadata":{"papermill":{"duration":0.109186,"end_time":"2021-08-13T07:17:23.608435","exception":false,"start_time":"2021-08-13T07:17:23.499249","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see a high amount of type 2 error. Due to class imbalance, the model is clearly trying to predict majority of the cases as class 0. Understanding how to handle class imbalance in classification models might be the key to winning this competition :) (hint!)","metadata":{"papermill":{"duration":0.068268,"end_time":"2021-08-13T07:17:23.788669","exception":false,"start_time":"2021-08-13T07:17:23.720401","status":"completed"},"tags":[]}},{"cell_type":"code","source":"precision_score(y_test, pipe_lr.predict(X_test[new_vars]))","metadata":{"papermill":{"duration":0.09953,"end_time":"2021-08-13T07:17:23.955991","exception":false,"start_time":"2021-08-13T07:17:23.856461","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recall_score(y_test, pipe_lr.predict(X_test[new_vars]))","metadata":{"papermill":{"duration":0.123943,"end_time":"2021-08-13T07:17:24.198308","exception":false,"start_time":"2021-08-13T07:17:24.074365","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"code","source":"from imblearn.pipeline import Pipeline as ImbPipeline\n# Define your pipeline\npipe_rf = ImbPipeline([\n    ('smote', SMOTE(random_state=42)),\n    ('pca', PCA(random_state=42)),\n    ('rf', RandomForestClassifier(random_state=42))\n])\n# Define the parameter grid for GridSearchCV\n# Define the parameter grid for GridSearchCV\nparam_grid = {\n    'smote__sampling_strategy': [0.5, 0.75, 1.0], \n    'rf__n_estimators': [20, 30, 50],  # Adjust as needed\n    'rf__max_depth': [10, 20, 30], # Adjust as needed\n    # Add other parameters you want to tune\n}\n\n# Create the GridSearchCV object\ngrid_search = GridSearchCV(pipe_rf, param_grid, cv=5, scoring='accuracy')  # Adjust scoring and cv as needed\n\n# Fit the model\ngrid_search.fit(X_train, y_train)\n\n# Access the best parameters and best estimator\nbest_params = grid_search.best_params_\nbest_model = grid_search.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\nrf.fit(pca_components, y_train)\n\nfeature_importances = pd.DataFrame({'col':['component_'+str(i) for i in range(len(rf.feature_importances_))], \n                                    'importance':rf.feature_importances_})\n\nplt.figure(figsize=(15,8))\nplt.xticks(rotation=45)\nplt.bar(feature_importances['col'], feature_importances['importance'])","metadata":{"papermill":{"duration":12.694325,"end_time":"2021-08-13T07:17:20.982792","exception":false,"start_time":"2021-08-13T07:17:08.288467","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe_rf = Pipeline(steps = [('imputation',imp),\n                         ('scaling',scale),\n                         ('pca',pca),\n                         ('model',rf)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe_rf.fit(X_train[new_vars], y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_score = pipe_rf.score(X_train[new_vars], y_train)\nprint(\"Training accuracy:\", train_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_score = pipe_rf.score(X_test[new_vars], y_test)\nprint(\"Testing accuracy:\", test_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Creating submission file\n\nFor submission, we need to make sure that the format is exactly the same as the sample.csv file. It contains 2 columns, id and churn_probability","metadata":{"papermill":{"duration":0.067672,"end_time":"2021-08-13T07:17:24.385001","exception":false,"start_time":"2021-08-13T07:17:24.317329","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sample.head()","metadata":{"papermill":{"duration":0.080814,"end_time":"2021-08-13T07:17:24.533810","exception":false,"start_time":"2021-08-13T07:17:24.452996","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The submission file should contain churn_probability values that have to be predicted for the unseen data provided (test.csv)","metadata":{"papermill":{"duration":0.068381,"end_time":"2021-08-13T07:17:24.670628","exception":false,"start_time":"2021-08-13T07:17:24.602247","status":"completed"},"tags":[]}},{"cell_type":"code","source":"unseen.head()","metadata":{"papermill":{"duration":0.098877,"end_time":"2021-08-13T07:17:24.838199","exception":false,"start_time":"2021-08-13T07:17:24.739322","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets first select the columns that we want to work with (or create them, if you have done any feature engineering)","metadata":{"papermill":{"duration":0.06952,"end_time":"2021-08-13T07:17:24.977086","exception":false,"start_time":"2021-08-13T07:17:24.907566","status":"completed"},"tags":[]}},{"cell_type":"code","source":"new_vars","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_vars = [col for col in new_vars if col != 'churn']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_data = unseen.set_index('id')[new_vars]\nsubmission_data.shape","metadata":{"papermill":{"duration":0.093498,"end_time":"2021-08-13T07:17:25.140420","exception":false,"start_time":"2021-08-13T07:17:25.046922","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, lets create a new column in the unseen dataset called churn_probability and use the model pipeline to predict the probabilities for this data","metadata":{"papermill":{"duration":0.069588,"end_time":"2021-08-13T07:17:25.279867","exception":false,"start_time":"2021-08-13T07:17:25.210279","status":"completed"},"tags":[]}},{"cell_type":"code","source":"unseen['churn_probability'] = pipe_rf.predict(submission_data)\noutput = unseen[['id','churn_probability']]\noutput.head()","metadata":{"papermill":{"duration":0.108369,"end_time":"2021-08-13T07:17:25.457668","exception":false,"start_time":"2021-08-13T07:17:25.349299","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, lets create a csv file out of this dataset, ensuring to set index=False to avoid an addition column in the csv.","metadata":{"papermill":{"duration":0.124336,"end_time":"2021-08-13T07:17:25.701056","exception":false,"start_time":"2021-08-13T07:17:25.576720","status":"completed"},"tags":[]}},{"cell_type":"code","source":"output.to_csv('submission.csv',index=False)","metadata":{"papermill":{"duration":0.12837,"end_time":"2021-08-13T07:17:25.914318","exception":false,"start_time":"2021-08-13T07:17:25.785948","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can now take this file and upload it as a submission on Kaggle.","metadata":{"papermill":{"duration":0.08214,"end_time":"2021-08-13T07:17:26.080507","exception":false,"start_time":"2021-08-13T07:17:25.998367","status":"completed"},"tags":[]}},{"cell_type":"code","source":"output['churn_probability'].value_counts()","metadata":{"papermill":{"duration":0.070782,"end_time":"2021-08-13T07:17:26.223247","exception":false,"start_time":"2021-08-13T07:17:26.152465","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}